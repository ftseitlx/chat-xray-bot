services:
  # Main application
  - type: web
    name: chat-xray-bot
    env: docker
    dockerfilePath: ./Dockerfile
    envVars:
      - key: PYTHON_VERSION
        value: 3.12.10
      - key: PORT
        value: 8000
      - key: OLLAMA_HOST
        value: llama2-ollama.onrender.com
      - key: OLLAMA_PORT
        value: 443
      - key: OLLAMA_URL
        value: https://llama2-ollama.onrender.com
      - key: OLLAMA_MODEL
        value: llama2:7b-chat
      - key: BOT_TOKEN
        sync: false
      - key: OPENAI_API_KEY
        sync: false
      - key: WEBHOOK_HOST
        sync: false
      - key: WEBHOOK_PATH
        value: /webhook
      - key: UPLOAD_RETENTION_HOURS
        value: 1
      - key: REPORT_RETENTION_HOURS
        value: 72
      - key: ENABLE_COST_TRACKING
        value: true
      - key: USE_LOCAL_LLM
        value: "true"
    healthCheckPath: /health

  # Ollama service with pre-built model
  - type: web
    name: llama2-ollama
    env: docker
    plan: standard
    dockerfilePath: ./Dockerfile.ollama
    envVars:
      - key: OLLAMA_HOST
        value: 0.0.0.0
      - key: OLLAMA_ORIGINS
        value: "*"
    healthCheckPath: /api/version 