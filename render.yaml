services:
  - type: web
    name: chat-xray-bot
    env: docker
    dockerfilePath: ./Dockerfile
    envVars:
      - key: PYTHON_VERSION
        value: 3.12.10
      - key: PORT
        value: 8000
      - key: OLLAMA_HOST
        value: llama2-ollama
      - key: OLLAMA_PORT
        value: 11434
      - key: BOT_TOKEN
        sync: false
      - key: OPENAI_API_KEY
        sync: false
      - key: WEBHOOK_HOST
        sync: false
      - key: WEBHOOK_PATH
        value: /webhook
      - key: UPLOAD_RETENTION_HOURS
        value: 1
      - key: REPORT_RETENTION_HOURS
        value: 72
      - key: ENABLE_COST_TRACKING
        value: true
      - key: USE_LOCAL_LLM
        value: "true"
      - key: OLLAMA_URL
        value: http://llama2-ollama:11434
      - key: OLLAMA_MODEL
        value: llama2:7b-chat
    dependsOn:
      - name: llama2-ollama
        type: private_service
    healthCheckPath: /health

  - type: private_service
    name: llama2-ollama
    env: docker
    plan: standard  # 4 GB RAM for llama2:7b-chat
    dockerfilePath: Dockerfile.ollama
    port: 11434
    autoDeploy: true
    healthCheckPath: /api/version
    healthCheckTimeout: 600
    disk:
      name: ollama-cache
      mountPath: /root/.ollama
      sizeGB: 20
    envVars:
      - key: OLLAMA_HOST
        value: 0.0.0.0
      - key: OLLAMA_ORIGINS
        value: "*" 